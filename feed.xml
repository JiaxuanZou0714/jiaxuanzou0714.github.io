<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jiaxuanzou0714.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jiaxuanzou0714.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-22T16:44:23+00:00</updated><id>https://jiaxuanzou0714.github.io/feed.xml</id><title type="html">Jiaxuan Zou</title><subtitle>Jiaxuan Zou (邹嘉轩)&apos;s academic homepage. </subtitle><entry><title type="html">Tensor Programs (一)：Feature Learning 的谱条件</title><link href="https://jiaxuanzou0714.github.io/blog/2026/spectral-condition-feature-learning/" rel="alternate" type="text/html" title="Tensor Programs (一)：Feature Learning 的谱条件"/><published>2026-02-14T17:00:00+00:00</published><updated>2026-02-14T17:00:00+00:00</updated><id>https://jiaxuanzou0714.github.io/blog/2026/spectral-condition-feature-learning</id><content type="html" xml:base="https://jiaxuanzou0714.github.io/blog/2026/spectral-condition-feature-learning/"><![CDATA[<blockquote> <p>这是 Tensor Programs 系列导读的第一篇。整个系列旨在向读者介绍 Greg Yang 发起的 <a href="https://thegregyang.com/">Tensor Programs</a> 研究计划——一个试图为深度学习中的宽度极限、feature learning 和超参数迁移提供统一数学基础的宏大框架。本文选择了 Greg Yang 本人推荐的入门论文 <em>A Spectral Condition for Feature Learning</em> [1] 作为起点。</p> </blockquote> <h2 id="0-why-tensor-programs">0. Why Tensor Programs?</h2> <p>深度学习的核心魔力在于 Feature Learning：模型能够从原始数据中自动学习到分层的、语义丰富的表示。这种能力使得神经网络超越了传统的核方法，让 LLM 的成功成为可能。</p> <p>然而，现在大多数的理论研究仍依赖于 NTK 这一套数学工具。NTK 描述了神经网络在 infinite-width 极限下的行为，它将神经网络视为一个固定的核函数，其预测值本质上是训练样本的核函数加权求和（即核回归，或者说初始化处的随机特征的线性回归）：</p> \[f(\boldsymbol{x}) = \sum_{i=1}^N \alpha_i K_{\text{NTK}}(\boldsymbol{x}, \boldsymbol{x}_i), \quad \text{其中 } K_{\text{NTK}}(\boldsymbol{x}, \boldsymbol{x}_i) = \langle \nabla f(\boldsymbol{x}; \boldsymbol{\theta}_0), \nabla f(\boldsymbol{x}_i; \boldsymbol{\theta}_0) \rangle.\] <p>NTK 框架的缺点在于，在它的setting下，训练后的权重仍然在初始化附近（也叫做lazy learning，因为这样才能够做泰勒展开的一阶近似）。此时，神经网络实际上是在初始化得到的随机特征上做线性回归，且特征在整个训练阶段都不改变，完全脱离真实情况下的 feature learning。</p> <p>而现实情况是，我们需要 feature learning，同时我们也在不断扩大模型宽度，但如果采用 NTK 的 setting，我们将失去 feature learning 能力（落入 lazy learning regime）。这引出了一个至关重要的问题：<strong>当我们想要训练宽度更大的模型（scale up）时，如何才能保持模型的 feature learning 能力？</strong></p> <p>这正是 Tensor Programs 系列研究试图回答的核心问题。Tensor Programs 不仅仅是一个简单的参数化技巧，它是一个宏大的数学框架，旨在精确描述神经网络计算在无限宽极限下的行为。在这个框架下，Greg Yang 等人推导出了一套在无限宽极限下能够保持特征学习能力的参数化方案——即著名的 Maximal Update Parametrization (μP)。</p> <p>换句话说，μP 只是 Tensor Programs 这一伟大数学框架的一个副产品（尽管是极其有用的副产品，它允许我们在小模型上调参并零样本迁移到大模型，即hyperparameter transfer）。要真正理解 μP 的本质，我们需要回到更基础的数学规律中去。而本文要介绍的论文 [1]，正提供了一个切入点，通过谱范数来直观理解 feature learning 保持条件，以及如何通过这个保持条件导出 μP。</p> <hr/> <h2 id="1-what-is-feature-learning">1. What is Feature Learning?</h2> <p>考虑一个 \(L\) 层 MLP。记第 \(\ell\) 层的隐藏表示为 \(\boldsymbol{h}_\ell(\boldsymbol{x}) \in \mathbb{R}^{n_\ell}\)，经过一步梯度下降后的变化为 \(\Delta \boldsymbol{h}_\ell(\boldsymbol{x})\)。我们希望神经网络发生 feature learning，说白了就是希望特征发生显著的改变，所以我们把 feature learning 定义如下。</p> <blockquote> <p>Feature Learning：对所有隐藏层 \(\ell\)，当各层宽度 \(n_\ell \to \infty\) 时： \(\|\boldsymbol{h}_\ell(\boldsymbol{x})\|_2 = \Theta(\sqrt{n_\ell}), \qquad \|\Delta \boldsymbol{h}_\ell(\boldsymbol{x})\|_2 = \Theta(\sqrt{n_\ell}).\)</p> </blockquote> <p>换句话说，特征向量每个分量的”典型大小”是 \(\Theta(1)\)（即常数量级，不随网络宽度 \(n\) 变化），这是合理的，因为常见的激活函数被设计成接受 \(O(1)\) 的输入并产生 \(O(1)\) 的输出。同时，训练中的更新量的分量也是 \(\Theta(1)\) 量级。这也是合理的——更大的更新会在宽度增大时爆炸（导致训练不稳定，发散），更小的更新会在宽度增大时消失（导致特征一直不变，即 lazy learning）。</p> <hr/> <h2 id="2-spectral-norm-vs-frobenius-norm">2. Spectral Norm vs. Frobenius Norm</h2> <p>在讨论 scaling 之前，需要区分两种矩阵范数。对一个 \(m \times n\) 矩阵 \(\boldsymbol{A}\)：</p> <ul> <li>谱范数：\(\|\boldsymbol{A}\|_* = \max_{\|\boldsymbol{v}\|_2=1} \|\boldsymbol{A}\boldsymbol{v}\|_2 = \sigma_{\max}(\boldsymbol{A})\)。</li> <li>Frobenius 范数：\(\|\boldsymbol{A}\|_F = \sqrt{\sum_{i,j} A_{ij}^2}\)。</li> </ul> <p>它们之间的关系是 \(\|\boldsymbol{A}\|_* \leq \|\boldsymbol{A}\|_F \leq \sqrt{\mathrm{rank}(\boldsymbol{A})} \cdot \|\boldsymbol{A}\|_*\)。</p> <p>关键差异在于：对于 iid Gaussian 随机矩阵 \(\boldsymbol{A} \in \mathbb{R}^{m \times n}\)（元素方差为 \(\sigma^2\)），</p> \[\|\boldsymbol{A}\|_F \approx \sigma \sqrt{mn}, \qquad \|\boldsymbol{A}\|_* \approx \sigma(\sqrt{m} + \sqrt{n}).\] <p>推导过程可见苏剑林的博客（<a href="https://spaces.ac.cn/archives/11335">随机矩阵的谱范数的快速估计</a>）。</p> <p>Frobenius 范数本质上在度量矩阵所有元素的”总能量”，它与矩阵的维度线性增长；而谱范数度量的是矩阵作为线性算子的最大放大倍数，更直接地反映了 \(\boldsymbol{A}\boldsymbol{v}\) 的行为。从这个角度来看，由于神经网络里有大量形如 \(\boldsymbol{A}\boldsymbol{v}\) 的线性运算，谱范数似乎是更合理的度量。</p> <p>这个差异也解释了为什么基于 Frobenius 范数或元素大小的 scaling 方案与基于谱范数的方案可能给出不同的结论。</p> <hr/> <h2 id="3-从-feature-learning-推导谱条件">3. 从 Feature Learning 推导谱条件</h2> <p>有了 feature learning 的目标定义和谱范数这个工具，我们可以开始推导：到底需要满足什么条件，才能保证 feature learning 发生？</p> <h3 id="31-前向传播scriptsizeboldsymbolh_ellboldsymbolx_2--thetasqrtn_ell">3.1 前向传播：\(\scriptsize\|\boldsymbol{h}_\ell(\boldsymbol{x})\|_2 = \Theta(\sqrt{n_\ell})\)</h3> <p>先考虑一个简化的 setting，考虑一个 \(L\) 层线性 MLP：\(\boldsymbol{h}_\ell(\boldsymbol{x}) = \boldsymbol{W}_\ell \boldsymbol{h}_{\ell-1}(\boldsymbol{x})\)，输入满足 \(\|\boldsymbol{x}\|_2 = \Theta(\sqrt{n_0})\)。</p> <p>我们的目标是让 \(\|\boldsymbol{h}_\ell\|_2 = \Theta(\sqrt{n_\ell})\)。每一层做的事情就是 \(\boldsymbol{h}_\ell = \boldsymbol{W}_\ell \boldsymbol{h}_{\ell-1}\)，而谱范数的次乘性告诉我们：</p> \[\|\boldsymbol{h}_\ell\|_2 = \|\boldsymbol{W}_\ell \boldsymbol{h}_{\ell-1}\|_2 \leq \|\boldsymbol{W}_\ell\|_* \cdot \|\boldsymbol{h}_{\ell-1}\|_2.\] <p>假设 \(\|\boldsymbol{h}_{\ell-1}\|_2 = \Theta(\sqrt{n_{\ell-1}})\) 已经成立（归纳假设），那么要让 \(\|\boldsymbol{h}_\ell\|_2 = \Theta(\sqrt{n_\ell})\)，我们需要线性算子的”放大倍数”恰好是：</p> \[\|\boldsymbol{W}_\ell\|_* = \Theta\!\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right).\] <p>这就是从 feature learning 的数值稳定性要求，自然推导出的对权重矩阵谱范数的约束。</p> <p>但仅有上界还不够——上面的不等式只说明了谱范数不能太大，否则特征会爆炸。次乘性的上界够紧吗？会不会实际上 \(\|\boldsymbol{W}_\ell \boldsymbol{h}_{\ell-1}\|_2 \ll \|\boldsymbol{W}_\ell\|_* \cdot \|\boldsymbol{h}_{\ell-1}\|_2\)，导致特征逐层消失？其实不会，我们有以下的观察。</p> <blockquote> <p>Claim 1：对于随机初始化的权重矩阵（高斯或半正交），当 fan-out \(\geq\) fan-in，即 \(n_\ell \geq n_{\ell-1}\) 时，有 \(\|\boldsymbol{W}_\ell \boldsymbol{h}_{\ell-1}\|_2 = \Theta(\|\boldsymbol{W}_\ell\|_* \cdot \|\boldsymbol{h}_{\ell-1}\|_2).\)</p> </blockquote> <p>这一点对高斯初始化用大数定律即可验证：\(\|\boldsymbol{W}_\ell \boldsymbol{h}\|^2 = \sum_{i=1}^{n_\ell} (\sum_j W_{ij} h_j)^2\)，每个 \(\sum_j W_{ij} h_j\) 的方差为 \(\sigma_\ell^2 \|\boldsymbol{h}\|^2\)，对 \(n_\ell\) 个独立项求和后浓度集中于期望 \(n_\ell \sigma_\ell^2 \|\boldsymbol{h}\|^2\)。结合 \(\|\boldsymbol{W}_\ell\|_* \approx \sigma_\ell(\sqrt{n_\ell} + \sqrt{n_{\ell-1}}) = \Theta(\sigma_\ell \sqrt{n_\ell})\)（当 \(n_\ell \geq n_{\ell-1}\)），这就保证了下界和上界同阶。</p> <p>于是，次乘性的上界在随机初始化下是紧的，\(\|\boldsymbol{W}_\ell\|_* = \Theta(\sqrt{n_\ell / n_{\ell-1}})\)，是充要条件。</p> <h3 id="32-梯度更新scriptsizedelta-boldsymbolh_ell_2--thetasqrtn_ell">3.2 梯度更新：\(\scriptsize\|\Delta \boldsymbol{h}_\ell\|_2 = \Theta(\sqrt{n_\ell})\)</h3> <p>Feature learning 不仅要求初始特征的量级正确，还要求训练过程中的特征更新 \(\Delta \boldsymbol{h}_\ell\) 也是 \(\Theta(\sqrt{n_\ell})\)。类似的推导会给出对 \(\Delta \boldsymbol{W}_\ell\) 的约束。</p> <p>对于 batch size 为 1 的梯度下降，第 \(\ell\) 层的权重更新为：</p> \[\Delta \boldsymbol{W}_\ell = -\eta_\ell \nabla_{\boldsymbol{W}_\ell} \mathcal{L}.\] <p>其中 \(\eta_\ell\) 是第 \(\ell\) 层的学习率。现在的问题是：\(\nabla_{\boldsymbol{W}_\ell} \mathcal{L}\) 长什么样？</p> <p>记反传信号 \(\boldsymbol{\delta}_\ell = \partial \mathcal{L} / \partial \boldsymbol{h}_\ell \in \mathbb{R}^{n_\ell}\)。由于前向传播是 \(\boldsymbol{h}_\ell = \boldsymbol{W}_\ell \boldsymbol{h}_{\ell-1}\)，对权重矩阵的第 \((i,j)\) 个元素用链式法则：</p> \[\frac{\partial \mathcal{L}}{\partial W_{\ell,ij}} = \frac{\partial \mathcal{L}}{\partial h_{\ell,i}} \cdot \frac{\partial h_{\ell,i}}{\partial W_{\ell,ij}} = \delta_{\ell,i} \cdot h_{\ell-1,j}.\] <p>写成矩阵形式，就是 \(\nabla_{\boldsymbol{W}_\ell} \mathcal{L} = \boldsymbol{\delta}_\ell \boldsymbol{h}_{\ell-1}^\top\)。代回更新公式：</p> \[\Delta \boldsymbol{W}_\ell = -\eta_\ell \boldsymbol{\delta}_\ell \boldsymbol{h}_{\ell-1}^\top.\] <p>这是一个秩一矩阵（两个向量的外积）。由于这是秩一矩阵，谱范数等于 Frobenius 范数：</p> \[\|\Delta \boldsymbol{W}_\ell\|_* = \|\Delta \boldsymbol{W}_\ell\|_F = \eta_\ell \|\boldsymbol{\delta}_\ell\|_2 \cdot \|\boldsymbol{h}_{\ell-1}\|_2.\] <p>这里有一个非常漂亮的性质。</p> <blockquote> <p>Claim 2：由于 \(\Delta \boldsymbol{W}_\ell\) 的右奇异向量恰好是 \(\boldsymbol{h}_{\ell-1}\)，所以 \(\|\Delta \boldsymbol{W}_\ell \cdot \boldsymbol{h}_{\ell-1}\|_2 = \|\Delta \boldsymbol{W}_\ell\|_* \cdot \|\boldsymbol{h}_{\ell-1}\|_2.\)</p> </blockquote> <p>次乘性在这里取等号——梯度更新和输入特征是完美对齐的。这说明训练的梯度更新恰好沿着最能影响当前特征的方向。</p> <p>因此，要让 \(\|\Delta \boldsymbol{h}_\ell\|_2 = \|\Delta \boldsymbol{W}_\ell \cdot \boldsymbol{h}_{\ell-1}\|_2 = \Theta(\sqrt{n_\ell})\)，结合 \(\|\boldsymbol{h}_{\ell-1}\|_2 = \Theta(\sqrt{n_{\ell-1}})\)，我们需要：</p> \[\|\Delta \boldsymbol{W}_\ell\|_* = \Theta\!\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right).\] <h3 id="33-总结spectral-scaling-condition">3.3 总结：Spectral Scaling Condition</h3> <p>综合前面两个方向的推导，我们得到了论文的核心结果：</p> <blockquote> <p>Spectral Scaling Condition：对于每一层 \(\ell = 1, \ldots, L\)，要求</p> \[\|\boldsymbol{W}_\ell\|_* = \Theta\!\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right), \qquad \|\Delta \boldsymbol{W}_\ell\|_* = \Theta\!\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right).\] </blockquote> <p>这个条件的含义现在非常清晰了：权重矩阵 \(\boldsymbol{W}_\ell \in \mathbb{R}^{n_\ell \times n_{\ell-1}}\) 作为将 \(n_{\ell-1}\) 维向量映射到 \(n_\ell\) 维向量的线性算子，其”放大倍数”（谱范数）需要恰好匹配输入输出的维度比。太大则特征爆炸，太小则特征消失或学习停滞。</p> <h3 id="34-推广从-toy-model-到真实网络">3.4 推广：从 Toy Model 到真实网络</h3> <p>虽然上述推导基于简化假设，但原论文 [1] 证明了 Spectral Scaling Condition 的结论在更广泛的情形下依然成立。</p> <ul> <li> <p>非线性激活函数：加入非线性激活 \(\boldsymbol{h}'_\ell = \phi(\boldsymbol{h}_\ell)\) 后，只要激活函数不改变特征向量的量级（即满足 \(\|\boldsymbol{h}'_\ell\|_2 = \Theta(\|\boldsymbol{h}_\ell\|_2)\)），那么 \(\Delta \boldsymbol{W}_\ell\) 依然是秩一矩阵，且满足完美对齐性质 \(\|\Delta \boldsymbol{W}_\ell \boldsymbol{h}'_{\ell-1}\|_2 = \|\Delta \boldsymbol{W}_\ell\|_* \cdot \|\boldsymbol{h}'_{\ell-1}\|_2\)。因此线性情况下的结论完全适用。</p> </li> <li> <p>批量大小 &gt; 1：当 \(B &gt; 1\) 时，更新量 \(\Delta \boldsymbol{W}_\ell = \frac{1}{B} \sum \Delta \boldsymbol{W}_\ell^{(i)}\) 不再是秩一矩阵，无法与所有输入向量完美对齐。但只要 \(B\) 与宽度 \(n\) 无关，且更新项之间不发生恶意的完全抵消，我们依然有 Scaling 意义上的对齐：</p> \[\|\Delta \boldsymbol{W}_\ell \boldsymbol{h}_\ell(\boldsymbol{x}_i)\|_2 = \Theta(\|\Delta \boldsymbol{W}_\ell\|_* \cdot \|\boldsymbol{h}_\ell(\boldsymbol{x}_i)\|_2)\] <p>这就足以保证谱缩放条件依然有效。有趣的是，论文观察到即使在大 batch size 下，更新矩阵依然保持数值上的低秩结构。</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-14/low_rank-480.webp 480w,/assets/img/post-02-14/low_rank-800.webp 800w,/assets/img/post-02-14/low_rank-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-14/low_rank.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="auto" height="auto" alt="更新矩阵的数值低秩结构" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>多步训练：梯度的演化依赖于”谱范数大小正确”和”传递特征大小正确”这两个性质。论文指出，只要更新量不与初始权重发生极端的完美抵消（\(\scriptsize\|\boldsymbol{W} + \Delta \boldsymbol{W}\|_* = \Theta(\|\boldsymbol{W}\|_* + \|\Delta \boldsymbol{W}\|_*)\)），那么一步更新后的权重将保持上述性质。归纳可知，feature learning 在后续训练步骤中依然成立。</p> </li> <li> <p>自适应优化器 (Adam)：对于 Adam 等逐元素处理梯度的优化器，论文在附录中证明，当宽度较大时，逐元素非线性处理能保持矩阵的 Frobenius 范数（最多相差常数倍），且梯度仍表现出类似独立向量外积的性质，因此结论同样适用。</p> </li> </ul> <hr/> <h2 id="4-从谱条件到-μp">4. 从谱条件到 μP</h2> <p>要满足 Spectral Scaling Condition，最直接的方法是对权重和梯度进行谱归一化。例如，我们可以强制设定：</p> \[\boldsymbol{W}_\ell = \sigma \sqrt{\frac{n_\ell}{n_{\ell-1}}} \frac{\boldsymbol{W}'_\ell}{\|\boldsymbol{W}'_\ell\|_*}, \qquad \Delta \boldsymbol{W}_\ell = -\eta \sqrt{\frac{n_\ell}{n_{\ell-1}}} \frac{\nabla_{\boldsymbol{W}_\ell} \mathcal{L}}{\|\nabla_{\boldsymbol{W}_\ell} \mathcal{L}\|_*}.\] <p>虽然这种方法能快速验证理论，但计算大规模矩阵的谱范数（最大奇异值）代价极其高昂，在实际训练中不可行。</p> <p>幸运的是，我们不需要显式计算谱范数。论文展示了可以通过分析随机矩阵的 scaling 规律，选择合适的逐层初始化方差 \(\sigma_\ell\) 和学习率 \(\eta_\ell\)，从而自动满足 Spectral Scaling Condition。这就是 μP 的本质。</p> <h3 id="41-初始化标度">4.1 初始化标度</h3> <p>假设 \(\boldsymbol{W}_\ell = \sigma_\ell \boldsymbol{W}'_\ell\)，其中 \(\boldsymbol{W}'_\ell\) 的元素为 iid 标准正态。由随机矩阵理论：</p> \[\|\boldsymbol{W}_\ell\|_* \approx \sigma_\ell (\sqrt{n_\ell} + \sqrt{n_{\ell-1}}).\] <p>要让 \(\|\boldsymbol{W}_\ell\|_* = \Theta(\sqrt{n_\ell / n_{\ell-1}})\)，需要：</p> \[\sigma_\ell = \Theta\!\left(\frac{\sqrt{n_\ell / n_{\ell-1}}}{\sqrt{n_\ell} + \sqrt{n_{\ell-1}}}\right) = \Theta\!\left(\frac{1}{n_{\ell-1}}\right) \quad \text{（当隐藏层等宽 $n_\ell = n$ 时）}.\] <h3 id="42-学习率标度">4.2 学习率标度</h3> <p>如何确定学习率 \(\eta_\ell\) 以满足 \(\|\Delta \boldsymbol{W}_\ell\|_* = \Theta(\sqrt{n_\ell / n_{\ell-1}})\)？这里的关键挑战在于确定梯度 \(\|\nabla_{\boldsymbol{W}_\ell} \mathcal{L}\|_*\) 的 scaling。</p> <p>我们可以通过对损失函数 \(\mathcal{L}\) 进行一阶泰勒展开来推导。 每一次梯度更新 \(\Delta \boldsymbol{W}_\ell\) 旨在引起输出 \(\Delta \boldsymbol{h}_L(\boldsymbol{x})\) 的变化，进而导致损失函数发生 \(\Theta(1)\) 量级的变化（\(\Delta \mathcal{L} = \Theta(1)\)）。</p> <p>利用迹内积的性质，损失变化量可以近似为：</p> \[\Delta \mathcal{L} \approx \langle \Delta \boldsymbol{W}_\ell, \nabla_{\boldsymbol{W}_\ell} \mathcal{L} \rangle = \Theta(\|\Delta \boldsymbol{W}_\ell\|_F \cdot \|\nabla_{\boldsymbol{W}_\ell} \mathcal{L}\|_F) = \Theta(\|\Delta \boldsymbol{W}_\ell\|_* \cdot \|\nabla_{\boldsymbol{W}_\ell} \mathcal{L}\|_*).\] <p>这里利用了我们在低秩更新下的观察：由于矩阵近似秩一（或低秩），其 Frobenius 范数与谱范数同阶。</p> <p>代入我们期望的 \(\Delta \mathcal{L} = \Theta(1)\) 和谱缩放条件 \(\|\Delta \boldsymbol{W}_\ell\|_* = \Theta(\sqrt{n_\ell / n_{\ell-1}})\)，我们可以直接解出梯度的 scaling：</p> \[\|\nabla_{\boldsymbol{W}_\ell} \mathcal{L}\|_* = \Theta(\sqrt{n_{\ell-1} / n_\ell}).\] <p>既然 \(\Delta \boldsymbol{W}_\ell = -\eta_\ell \nabla_{\boldsymbol{W}_\ell} \mathcal{L}\)，要满足 \(\|\Delta \boldsymbol{W}_\ell\|_* = \Theta(\sqrt{n_\ell / n_{\ell-1}})\)，学习率必须设定为：</p> \[\eta_\ell = \frac{\|\Delta \boldsymbol{W}_\ell\|_*}{\|\nabla_{\boldsymbol{W}_\ell} \mathcal{L}\|_*} = \Theta\left(\frac{n_\ell}{n_{\ell-1}}\right).\] <p>这给出了 μP 学习率 scaling 的直观解释：对于标准的 \(n_\ell = n\) 隐藏层，学习率应为 \(\Theta(1)\)；而对于输出层（假设 \(n_L=1\)），学习率应为 \(\Theta(1/n)\)。</p> <h3 id="43-spectral-parametrization">4.3 Spectral Parametrization</h3> <p>将初始化和学习率的推导结果结合起来，论文总结出了 Spectral Parametrization，也是该论文的主要贡献之一。</p> <blockquote> <p>如果每一层 $\ell$ 的初始化缩放和学习率按照以下方式选择，则谱缩放条件成立且实现特征学习： \(\sigma_\ell = \Theta \left( \frac{1}{\sqrt{n_{\ell-1}}} \min \left\{ 1, \sqrt{\frac{n_\ell}{n_{\ell-1}}} \right\} \right), \qquad \eta_\ell = \Theta \left( \frac{n_\ell}{n_{\ell-1}} \right).\)</p> </blockquote> <p>这个统一的公式涵盖了所有层的情况：</p> <ul> <li>对于隐藏层（通常 $n_\ell \approx n_{\ell-1}$），$\sigma_\ell = \Theta(1/n_{\ell-1})$，$\eta_\ell = \Theta(1)$。</li> <li>对于输出层（$n_\ell \ll n_{\ell-1}$，例如 $n_L=1$），$\sigma_\ell = \Theta(1/n_{\ell-1})$，$\eta_\ell = \Theta(1/n_{\ell-1})$。</li> </ul> <p>这与 μP 表（Yang et al., 2021 的 Table 3）完全一致。换句话说，谱 scaling 条件提供了 μP 的一种等价但更直观的推导方式。</p> <hr/> <h2 id="5-与其他参数化方案的对比">5. 与其他参数化方案的对比</h2> <h3 id="51-standard-parametrizationsp">5.1 Standard Parametrization（SP）</h3> <p>主流的 Kaiming/Xavier/LeCun 初始化使用 \(\sigma_\ell = \Theta(1/\sqrt{n_{\ell-1}})\)，配合与宽度无关的学习率。</p> <p>这意味着：</p> \[\|\boldsymbol{W}_\ell\|_* \approx \frac{1}{\sqrt{n_{\ell-1}}} (\sqrt{n_\ell} + \sqrt{n_{\ell-1}}) = \Theta\!\left(1 + \sqrt{\frac{n_\ell}{n_{\ell-1}}}\right).\] <p>当 \(n_\ell \gg n_{\ell-1}\) 时这远大于 \(\sqrt{n_\ell / n_{\ell-1}}\)，但更关键的是，SP 在输出层（fan-out \(\ll\) fan-in）的谱范数偏大，导致宽度变大时输出可能发散。</p> <p>而 SP 使用固定学习率（与宽度无关），对于宽的隐藏层来说，学习率实际上偏小——更新的谱范数随宽度衰减，feature learning 不足。</p> <h3 id="52-neural-tangent-parametrizationntp">5.2 Neural Tangent Parametrization（NTP）</h3> <p>NTP 将权重参数化为 \(\boldsymbol{W}_\ell / \sqrt{n_{\ell-1}}\)，使用与宽度无关的学习率。可以验证它等价于 \(\sigma_\ell = \Theta(1/\sqrt{n_{\ell-1}})\)，\(\eta_\ell = \Theta(1/n_{\ell-1})\)。</p> <p>输出层的 \(\sigma_L\) 比 μP 大了 \(\sqrt{n_{L-1}}\) 倍，这通过反向传播放大了所有中间层的梯度；然而，\(\eta_\ell = 1/n_{\ell-1}\) 的过小学习率又将这个放大效应压回去了。最终结果是：</p> \[\|\Delta \boldsymbol{W}_\ell\|_* \propto \frac{\sqrt{n_{L-1}}}{n_{\ell-1}} \to 0 \quad (n \to \infty).\] <p>权重更新的谱范数随宽度衰减至零——这正是 lazy learning / kernel regime 的特征：特征冻结，网络行为退化为 NTK。</p> <h3 id="53-小结">5.3 小结</h3> <table class="table table-striped"> <thead> <tr> <th>方案</th> <th>初始化 \(\sigma_\ell\)（隐藏层）</th> <th>学习率 \(\eta_\ell\)（隐藏层）</th> <th>Feature Learning?</th> </tr> </thead> <tbody> <tr> <td>SP</td> <td>\(1/\sqrt{n}\)</td> <td>\(\Theta(1)\)</td> <td>✗</td> </tr> <tr> <td>NTP</td> <td>\(1/\sqrt{n}\)</td> <td>\(1/n\)</td> <td>✗</td> </tr> <tr> <td>μP / Spectral</td> <td>\(1/n\)</td> <td>\(\Theta(1)\)</td> <td>✓</td> </tr> </tbody> </table> <hr/> <h2 id="6-实验验证">6. 实验验证</h2> <p>为了验证上述理论推导，论文在不同宽度的 MLP 上进行了实验。下图展示了在 NTP 和 μP 缩放下，网络内部特征和权重变化的 scaling 行为。横坐标都是网络宽度 \(n\)，纵坐标都是特征变化量和权重变化量。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-14/experiments-480.webp 480w,/assets/img/post-02-14/experiments-800.webp 800w,/assets/img/post-02-14/experiments-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-14/experiments.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="auto" height="auto" alt="不同参数化方案下的训练表现" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>可以看到：</p> <ol> <li>特征变化量（左图）：在 μP 缩放下，特征变化量 \(\frac{\|\boldsymbol{h}_2(\boldsymbol{x}) - \boldsymbol{h}_2^0(\boldsymbol{x})\|_2}{\|\boldsymbol{h}_2^0(\boldsymbol{x})\|_2}\) 保持常数级别 \(\Theta(1)\)，与宽度无关；而在 NTP 缩放下，特征变化量随宽度增加呈 \(n^{-1/2}\) 衰减。这意味着在 NTP 缩放下，随着模型变宽，特征学习会逐渐消失，最终退化为 Lazy Regime。</li> <li>权重变化量（右图）：在 μP 缩放下，权重的谱范数变化 \(\frac{\|\boldsymbol{W}_2 - \boldsymbol{W}_2^0\|_*}{\|\boldsymbol{W}_2^0\|_*}\) 也不随宽度衰减（保持 \(\Theta(1)\)），而 NTP 缩放下则显著衰减。</li> </ol> <p>这证实了只有 μP 能够在大宽度的极限下保持非平凡的特征学习。</p> <hr/> <h2 id="7-如何理解唯一的最大缩放">7. 如何理解唯一的最大缩放</h2> <p>Spectral Scaling Condition 给出的是唯一的最大缩放（maximal scaling）。</p> <p>具体而言，如果任何 \(\|\boldsymbol{W}_\ell\|_*\) 或 \(\|\Delta \boldsymbol{W}_\ell\|_*\) 超过了 \(\Theta(\sqrt{n_\ell / n_{\ell-1}})\)，那么随着宽度增大，训练会发散。反过来，过小的 scaling 会导致 feature learning 不充分，甚至落入 lazy learning regime。μP（谱条件）正是让每一层的 feature learning 都尽可能充分的唯一解。</p> <p>这也是”maximal update parametrization”中”maximal”一词的来源。</p> <hr/> <h2 id="8-与-tensor-programs-系列的关系">8. 与 Tensor Programs 系列的关系</h2> <p>这篇论文展示了一种绕过 Tensor Programs 形式化体系的、用基本线性代数推导 μP 的方法。但需要注意的是，Tensor Programs 的真正威力在于它的普适性：它可以处理任意架构（不只是 MLP）、任意优化器（不只是 SGD）、以及训练的全过程（不只是一步）。</p> <table class="table table-striped"> <thead> <tr> <th>问题</th> <th>本文方法</th> <th>Tensor Programs</th> </tr> </thead> <tbody> <tr> <td>适用架构</td> <td>MLP（可推广）</td> <td>任意 TP 可表达的架构</td> </tr> <tr> <td>适用优化器</td> <td>SGD（可推广到 Adam）</td> <td>任意自适应优化器</td> </tr> <tr> <td>训练步数</td> <td>一步 → 多步</td> <td>无穷步（极限定理）</td> </tr> <tr> <td>推导难度</td> <td>基本线性代数</td> <td>需要 Master Theorem</td> </tr> </tbody> </table> <p>在后续的系列文章中，我们将逐步深入 Tensor Programs 的形式化框架，理解 Master Theorem 如何给出任意神经网络计算的无穷宽极限。</p> <hr/> <h2 id="参考文献">参考文献</h2> <p>[1] Bernstein, J., Newhouse, L., Lee, J., Yang, G. (2024). A Spectral Condition for Feature Learning. <em>arXiv preprint arXiv:2310.17813</em>.</p> <p>[2] Yang, G. &amp; Hu, E. J. (2021). Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks. <em>ICML 2021</em>. arXiv:2011.14522.</p> <p>[3] Yang, G., Hu, E. J., Babuschkin, I., et al. (2022). Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer. <em>arXiv preprint arXiv:2203.03466</em>.</p> <p>[4] Yang, G., Schnabel, T., Li, Z., &amp; Du, S. S. (2023). Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks. <em>arXiv preprint arXiv:2310.02244</em>.</p> <h2 id="引用">引用</h2> <p>如果您需要引用本文，请参考：</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zou2026spectral</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Tensor Programs (一)：Feature Learning 的谱条件}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zou, Jiaxuan}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{Jiaxuan's Blog}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://jiaxuanzou0714.github.io/blog/2026/spectral-condition-feature-learning/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="deep-learning"/><category term="deep-learning"/><category term="tensor-programs"/><category term="muP"/><category term="feature-learning"/><summary type="html"><![CDATA[本文介绍 Greg Yang 的 Tensor Programs 系列的入门论文——A Spectral Condition for Feature Learning，从谱范数的视角推导出 feature learning 所需的 scaling 条件，并由此重新推导 maximal update parametrization（μP）。]]></summary></entry><entry><title type="html">从 Gated DeltaNet 到 Kaczmarz</title><link href="https://jiaxuanzou0714.github.io/blog/2026/kaczmarz/" rel="alternate" type="text/html" title="从 Gated DeltaNet 到 Kaczmarz"/><published>2026-02-08T10:00:00+00:00</published><updated>2026-02-08T10:00:00+00:00</updated><id>https://jiaxuanzou0714.github.io/blog/2026/kaczmarz</id><content type="html" xml:base="https://jiaxuanzou0714.github.io/blog/2026/kaczmarz/"><![CDATA[<p>很多linear attention，都可以被看成在维护一个 Key 到 Value 的线性映射矩阵 $S_t$。Gated DeltaNet [1] 的特别之处在于：它不直接写入 $K_t^\top V_t$，而是写入预测残差，并用门控控制遗忘与写入强度。</p> <h2 id="1-gated-deltanet-的更新公式">1. Gated DeltaNet 的更新公式</h2> <p>在时刻 $t$，给定行向量形式的 $K_t \in \mathbb{R}^{1\times d_k}$ 和 $V_t \in \mathbb{R}^{1\times d_v}$，以及记忆矩阵 $S_t \in \mathbb{R}^{d_k\times d_v}$。</p> <p>Gated DeltaNet 的核心递推公式为：</p> \[\begin{aligned} e_t &amp;= V_t - K_t S_{t-1} \\ S_t &amp;= \alpha_t S_{t-1} + \beta_t K_t^\top e_t \end{aligned}\] <p>其中门控通常由输入 $X$ 产生：</p> <ul> <li>$\beta_t = \sigma(\mathrm{Linear}_\beta(X))$ 代表写入强度。</li> <li>$\alpha_t \in (0,1]$ 是遗忘/衰减系数，用来控制对旧记忆 $S_{t-1}$ 的保留比例。</li> </ul> <h2 id="2-online-learning-视角">2. Online learning 视角</h2> <p>我们可以把 $S$ 视作多输出线性回归的权重：$\hat V_t = K_t S$。对单样本的在线岭回归损失函数为：</p> \[\ell_t(S)=\frac12\|V_t-K_tS\|_F^2+\frac{\lambda_t}{2}\|S\|_F^2\] <p>其梯度为：</p> \[\nabla_S\ell_t(S)=-K_t^\top e_t+\lambda_t S\] <p>如果做一步梯度下降（学习率为 $\beta_t$）：</p> \[\begin{aligned} S_t &amp;=S_{t-1}-\beta_t\nabla_S\ell_t(S_{t-1})\\ &amp;=(1-\beta_t\lambda_t)S_{t-1}+\beta_tK_t^\top(V_t-K_tS_{t-1}) \end{aligned}\] <p>对比 Gated DeltaNet 的形式：</p> \[S_t=\alpha_tS_{t-1}+\beta_tK_t^\top(V_t-K_tS_{t-1})\] <p>我们可以得到精确的对应关系：$\alpha_t = 1-\beta_t\lambda_t$。而在实现中，通常用 $\alpha_t=\exp(g_t)$ 来保证数值稳定性。</p> <h2 id="3-引入-kaczmarz-算法">3. 引入 Kaczmarz 算法</h2> <p>在约束视角下，SGD 不是唯一的优化方法。当我们更强调满足约束的几何意义时，可以把每个 token 看成一个线性约束 $K_tS = V_t$。</p> <p>Kaczmarz 的一步更新可以定义为：在满足当前约束的集合里，找一个离旧解最近的 $S$，即最小改动投影：</p> \[S_t=\arg\min_S\frac12\|S-S_{t-1}\|_F^2\quad\text{s.t.}\quad K_tS=V_t\] <p>我们可以通过拉格朗日乘子法来求解。构造拉格朗日函数：</p> \[\mathcal{L}(S,\lambda)=\frac12\|S-S_{t-1}\|_F^2+\lambda(K_tS-V_t)\] <p>对 $S$ 求导并令其为零：</p> \[\nabla_S\mathcal{L}=(S-S_{t-1})+K_t^\top\lambda=0 \Rightarrow S=S_{t-1}-K_t^\top\lambda\] <p>将 $S$ 代回约束条件 $K_tS=V_t$：</p> \[K_t(S_{t-1}-K_t^\top\lambda)=V_t \Rightarrow K_tS_{t-1}-\|K_t\|_2^2\lambda=V_t\] <p>解得：</p> \[\lambda=-\frac{V_t-K_tS_{t-1}}{\|K_t\|_2^2} = -\frac{e_t}{\|K_t\|_2^2}\] <p>因此得到 Kaczmarz 递推公式：</p> \[\boxed{ S_t=S_{t-1}+\frac{1}{\|K_t\|_2^2}\,K_t^\top e_t }\] <h2 id="4-几何解释">4. 几何解释</h2> <p>Kaczmarz 算法具有明确的几何意义：</p> <ol> <li>投影意义：Kaczmarz 的更新步骤将 $S_{t-1}$ 投影到超平面 ${S:K_tS=V_t}$ 上。</li> <li>误差消除：当使用硬投影时，更新后的 $S_t$ 满足当前约束，即 $K_tS_t=V_t$。</li> <li>最优步长：它可视作最优步长的一步梯度法。若对瞬时平方误差沿梯度方向进行精确线搜索，最优步长为 $1/|K_t|^2$。</li> <li>QK Norm 视角：这个归一化因子也解释了如今 Attention / Linear Attention 在实现层面会加的一个 trick——QK Norm，我们在这里就显式地引入了它。</li> </ol> <h2 id="5-推广到不一致约束">5. 推广到不一致约束</h2> <p>在现实情况中，约束往往是不一致的或带有噪声的，不存在一个 $S$ 能同时满足所有约束。硬投影可能会导致模型抖动或过拟合当前样本。</p> <p>因此常用 Relaxed Kaczmarz（或 Damped Kaczmarz），在分子引入松弛系数 $\rho_t$，并在分母加数值稳定项 $\varepsilon$：</p> \[\boxed{ S_t = S_{t-1} + \frac{\rho_t}{\|K_t\|_2^2+\varepsilon}\,K_t^\top\bigl(V_t-K_tS_{t-1}\bigr) }\] <p>该更新公式满足以下性质：</p> \[K_tS_t=(1-\rho_t)K_tS_{t-1}+\rho_tV_t\] <p>即新的预测值是旧预测值与目标值之间的线性插值。</p> <p>若结合 Gated DeltaNet 的遗忘机制，可采用先遗忘后投影的更新方式：</p> \[\tilde S_{t-1}=\alpha_tS_{t-1},\qquad S_t=\tilde S_{t-1}+\frac{\rho_t}{\|K_t\|^2+\varepsilon}K_t^\top\bigl(V_t-K_t\tilde S_{t-1}\bigr)\] <h2 id="6-与-longhorn-的联系">6. 与 Longhorn 的联系</h2> <p>Longhorn [2] 采用了更平滑的更新策略，将硬约束投影替换为近端目标优化：</p> \[S_t=\arg\min_S\frac12\|S-S_{t-1}\|_F^2+\frac{\gamma_t}{2}\|V_t-K_tS\|_F^2\] <p>其显式解为：</p> \[\boxed{ S_t = S_{t-1} + \frac{\gamma_t}{1+\gamma_t\|K_t\|_2^2} K_t^\top\bigl(V_t-K_tS_{t-1}\bigr) }\] <p>这个系数可以重写为：</p> \[\frac{\gamma_t}{1+\gamma_t\|K_t\|^2} = \frac{1}{\|K_t\|^2+\frac{1}{\gamma_t}}\] <p>只需令 $\varepsilon_t = 1/\gamma_t$，这表明 Longhorn 类似于 Kaczmarz。</p> <h2 id="7-方法对比表">7. 方法对比表</h2> <table class="table table-striped"> <thead> <tr> <th>方法</th> <th>状态更新</th> <th>有效步长</th> </tr> </thead> <tbody> <tr> <td>Gated DeltaNet</td> <td>$S_t=\alpha_tS_{t-1}+\beta_tK_t^\top e_t$</td> <td>$\beta_t$</td> </tr> <tr> <td>Kaczmarz</td> <td>$S_t=S_{t-1}+\frac{1}{|K_t|^2}K_t^\top e_t$</td> <td>$\frac{1}{|K_t|^2}$</td> </tr> <tr> <td>Relaxed Kaczmarz</td> <td>$S_t=S_{t-1}+\frac{\rho_t}{|K_t|^2+\varepsilon}K_t^\top e_t$</td> <td>$\frac{\rho_t}{|K_t|^2+\varepsilon}$</td> </tr> <tr> <td>Longhorn / Prox</td> <td>$S_t=S_{t-1}+\frac{\gamma_t}{1+\gamma_t|K_t|^2}K_t^\top e_t$</td> <td>$\frac{1}{|K_t|^2+1/\gamma_t}$</td> </tr> </tbody> </table> <h2 id="8-实验">8. 实验</h2> <p>我们在 SlimPajama 数据集上训练了 1 亿个 token，并在验证集上绘制了困惑度（ppl）与 Loss 随 token horizon 变化的曲线。图中 @1x 与 @2x 分别对应序列长度 2048 与 4096 的评估结果。</p> <figure> <picture> <img src="/assets/img/post-02-08/result.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">在 SlimPajama 数据集上的实验结果。@1x 代表序列长度为 2048，@2x 代表序列长度为 4096。</figcaption> </figure> <p>若 Longhorn 可视为 Kaczmarz 的某种形式，则 Relaxed Kaczmarz 理应更优，实验亦证实了这一点。同时，gated deltanet 使用了 qk norm 的 trick，如果我们也补充上 q norm 这个 trick，可以得到 relaxed kaczmarz + q norm 和 gated deltanet 不相上下的结果。</p> <h2 id="参考文献">参考文献</h2> <p>[1] Yang, S., Kautz, J., &amp; Hatamizadeh, A. (2025). Gated Delta Networks: Improving Mamba2 with Delta Rule. <em>arXiv preprint arXiv:2412.06464</em>.</p> <p>[2] Liu, B., Wang, R., Wu, L., Feng, Y., Stone, P., &amp; Liu, Q. (2024). Longhorn: State Space Models are Amortized Online Learners. <em>arXiv preprint arXiv:2407.14207</em>.</p> <h2 id="引用">引用</h2> <p>如果您需要引用本文，请参考：</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zou2026kaczmarz</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{从 Gated DeltaNet 到 Kaczmarz}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zou, Jiaxuan}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{Jiaxuan's Blog}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://jiaxuanzou0714.github.io/blog/2026/kaczmarz/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="deep-learning"/><category term="deep-learning"/><category term="optimization"/><category term="linear-attention"/><summary type="html"><![CDATA[本文从 Gated DeltaNet 的在线学习形式出发，并引入 Kaczmarz 算法作为 SGD 的替代方案，分析了其几何意义及与 Longhorn 的联系。]]></summary></entry><entry><title type="html">如何对齐不同初始化大小下的 Data scaling 曲线</title><link href="https://jiaxuanzou0714.github.io/blog/2026/data-scaling-and-std/" rel="alternate" type="text/html" title="如何对齐不同初始化大小下的 Data scaling 曲线"/><published>2026-02-01T10:00:00+00:00</published><updated>2026-02-01T10:00:00+00:00</updated><id>https://jiaxuanzou0714.github.io/blog/2026/data-scaling-and-std</id><content type="html" xml:base="https://jiaxuanzou0714.github.io/blog/2026/data-scaling-and-std/"><![CDATA[<h2 id="现象">现象</h2> <p>我们在 上一篇 blog (<a href="/blog/2025/scaling-law/">Can We Derive Scaling Law From First Principles</a>) 中讨论了 scaling law 到底是如何产生的。我们在其预印本中增加了实验章节，其中有一系列图展示了不同 $\alpha$ 下的 data scaling 曲线，比如其中一张图，展示了在数据受限的情况下，loss 与 datasize 的关系</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/data-scaling-0p01-480.webp 480w,/assets/img/post-02-01/data-scaling-0p01-800.webp 800w,/assets/img/post-02-01/data-scaling-0p01-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/data-scaling-0p01.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="50%" height="auto" alt="替代文本" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>为了确保发生 feature learning，在这张图里，我们设定初始化的 std=0.01。但是如果我们把 std 增大一些，设置成 0.05，会发生什么呢？结果如下</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/data-scaling-0p05-480.webp 480w,/assets/img/post-02-01/data-scaling-0p05-800.webp 800w,/assets/img/post-02-01/data-scaling-0p05-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/data-scaling-0p05.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="50%" height="auto" alt="替代文本" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>我们可以看见，std=0.05 时，data scaling 的曲线发生了偏移，那如果再增大一些呢？设置成 0.1，结果如下</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/data-scaling-0p1-480.webp 480w,/assets/img/post-02-01/data-scaling-0p1-800.webp 800w,/assets/img/post-02-01/data-scaling-0p1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/data-scaling-0p1.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="50%" height="auto" alt="替代文本" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>随着初始化 std 增大，empirical 的直线逐渐偏离理论预测的直线。那如果我们把 empirical slope 关于 std 的曲线画出来会是什么样子？结果如下</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure id="fig:std-slope"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/fix_lr-480.webp 480w,/assets/img/post-02-01/fix_lr-800.webp 800w,/assets/img/post-02-01/fix_lr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/fix_lr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">empirical slope 与初始化 std 的关系</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure id="fig:all-curves-fix"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/all-curves-fix-480.webp 480w,/assets/img/post-02-01/all-curves-fix-800.webp 800w,/assets/img/post-02-01/all-curves-fix-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/all-curves-fix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">不同初始化 std 下的 data scaling 曲线</figcaption> </figure> </div> </div> <p>这个结果就很有趣了，可能的原因有很多，比如 std 太大导致模型进入了 lazy learning regime；或者 std 增大带来了优化过程的不稳定（这一种可能原因有待商榷，因为我测试了不同 epoch 数，结果是<a href="#fig:std-slope">上图</a>中的曲线仍能够稳定复现）。但我们可以先不考虑其诱因，先来考虑如何对齐不同初始化大小下的 data scaling 曲线。</p> <h2 id="前提定义">前提定义</h2> <p>我们考虑一个两层 ReLU 网络，其前向计算写成 $g(W_1,W_2;x)=W_2\phi(W_1 x)$，其中$\phi=\mathrm{ReLU}$，且满足正齐次性：对任意 $c&gt;0$,</p> \[\phi(cu)=c\,\phi(u).\] <p>输入 $x\in\mathbb R^K$ 被设为 One-hot 或归一化向量且满足 $\lVert x\rVert^2=1$，隐藏层先得到预激活 $h=W_1 x$ 再经 ReLU 得到激活值 $a=\phi(h)=\mathrm{ReLU}(h)$，最终输出 $y=W_2 a\in\mathbb R^K$，而隐藏层宽度记为 $N$。权重矩阵 $W_1,W_2$ 均按 $\mathcal N(0,\sigma^2)$ 独立同分布地初始化。对于任意元素均值为 $0$、方差为 $\sigma^2$ 的矩阵 $A\in\mathbb R^{d_{\mathrm{out}}\times d_{\mathrm{in}}}$，其 Frobenius 范数的期望平方满足 $\mathbb E[\lVert A\rVert_F^2]=d_{\mathrm{out}}\cdot d_{\mathrm{in}}\cdot\sigma^2$，因而量级上 $\lVert A\rVert_F \propto \sigma$。</p> <h2 id="尝试-1-失败">尝试 1 (失败)</h2> <p>固定学习率 lr 的情况下，增大初始化 std，虽然绝对步长不变，但是相对于$\lVert W\rVert_F$的步长就小了，过小的权重更新量会导致模型落入 lazy learning regime。从这点出发我们需要计算 相对更新比率 R：</p> \[\mathcal{R} = \frac{\lVert\Delta W\rVert_F}{\lVert W\rVert_F} = \frac{\lVert\eta \cdot \nabla W\rVert_F}{\lVert W\rVert_F}\] <p>这是分母部分，非常直接。 对于第二层权重 $W_2 \in \mathbb{R}^{K \times N}$：</p> \[\mathbb{E}[\lVert W_2\rVert_F^2] = K \cdot N \cdot \sigma^2\] <p>取均方根作为量级估计：</p> \[\lVert W_2\rVert_F \approx \sqrt{KN} \cdot \sigma \propto \sigma\] <p>下面我们估计$\lVert\nabla W_2\rVert_F$。预激活 $h$ 的每个元素 $h_j$ 服从 $\mathcal{N}(0,\sigma^2$。$\text{Var}(h_j) = \sigma^2$。对于激活层输出 a：$a_j = \text{ReLU}(h_j)$。由于 ReLU 将负半轴置零，二阶矩减半：</p> \[\mathbb{E}[a_j^2] = \frac{1}{2} \mathbb{E}[h_j^2] = \frac{1}{2} \sigma^2\] <p>则激活层的向量范数 $\lVert a\rVert^2 = \sum_{j=1}^N a_j^2$ 的期望为：</p> \[\mathbb{E}[\lVert a\rVert^2] = N \cdot \frac{1}{2}\sigma^2 \implies \lVert a\rVert \propto \sqrt{N}\sigma\] <hr/> <p>对于第二层输出 y：$y_k = \sum_{j=1}^N W_{2, kj} a_j$。假设 $W_2$ 与 $a$ 独立，且 $\mathbb{E}[W_2]=0$。则</p> \[\text{Var}(y_k) = \sum_{j=1}^N \text{Var}(W_{2, kj} a_j) = \sum_{j=1}^N \mathbb{E}[W_{2, kj}^2] \mathbb{E}[a_j^2]\] <p>代入已知项：</p> \[\text{Var}(y_k) = N \cdot (\sigma^2) \cdot (\frac{1}{2}\sigma^2) = \frac{N}{2} \sigma^4\] <p>则$\lVert y\rVert \approx \sqrt{\frac{N}{2}} \sigma^2 \propto \sqrt{N} \sigma^2$</p> <hr/> <p>损失函数 $L = \frac{1}{2} \lVert y - t\rVert^2$。梯度公式为：</p> \[\nabla_{W_2} L = (y - t) \cdot a^T.\] <p>当 $\sigma$ 较大时，初始输出 $\lVert y\rVert$ 远大于目标 $\lVert t\rVert$（在我们的 setting 里是 One-hot，量级为 1）。因此，误差项 $\epsilon = y - t \approx y$。现在计算梯度矩阵 $\nabla_{W_2} L$ 的范数：</p> \[\lVert\nabla_{W_2} L\rVert_F = \lVert(y - t) a^T\rVert_F = \lVert y - t\rVert_2 \cdot \lVert a\rVert_2.\] <p>代入步骤 2 的结果，$\lVert y - t\rVert \approx \lVert y\rVert \propto \sqrt{N} \sigma^2$，$\lVert a\rVert \propto \sqrt{N} \sigma$。相乘得到梯度量级：</p> \[\lVert\nabla W_2\rVert_F \approx (\sqrt{N} \sigma^2) \cdot (\sqrt{N} \sigma) = N \sigma^3.\] <hr/> <p>现在我们将上述结果代入相对更新率公式：</p> \[\mathcal{R} = \frac{\lVert\Delta W_2\rVert_F}{\lVert W_2\rVert_F} = \frac{\eta \lVert\nabla W_2\rVert_F}{\lVert W_2\rVert_F}\] <p>代入量级关系：</p> \[\mathcal{R} \approx \frac{\eta \cdot (N \sigma^3)}{\sqrt{KN} \cdot \sigma}.\] <p>化简（忽略常数 $K$）：</p> \[\mathcal{R} \propto \frac{\eta \sigma^3}{\sigma} = \eta \sqrt{N} \sigma^2.\] <p>为了保持相对更新量恒定，我们需要 $\mathcal{R} = \text{Constant}$，即：</p> \[\eta \sigma^2 = C \implies \eta \propto \frac{1}{\sqrt{N} \sigma^2}.\] <p>由于在我们的实验里，$N$是固定的，只改变初始化 std 于是只需要 $\eta \propto \frac{1}{\sigma^2}$. 于是我立刻在代码里实现了这一个调整。结果如下</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure id="fig:std-slope"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/ada_lr-480.webp 480w,/assets/img/post-02-01/ada_lr-800.webp 800w,/assets/img/post-02-01/ada_lr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/ada_lr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">empirical slope 与初始化 std 的关系</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure id="fig:all-curves-ada"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/all-curves-ada-480.webp 480w,/assets/img/post-02-01/all-curves-ada-800.webp 800w,/assets/img/post-02-01/all-curves-ada-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/all-curves-ada.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">不同初始化 std 下的 data scaling 曲线</figcaption> </figure> </div> </div> <p>我们放在同一图里对比一下：</p> <figure id="fig:compare-fix-ada"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/compare-fix-ada-480.webp 480w,/assets/img/post-02-01/compare-fix-ada-800.webp 800w,/assets/img/post-02-01/compare-fix-ada-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/compare-fix-ada.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">有无自适应学习率的empirical slope对比</figcaption> </figure> <p>可以看见，自适应学习率 $\eta \propto \frac{1}{\sigma^2}$ 确实缓解了一点初始化 std 对 scaling 曲线 empirical slope 的影响。但在初始化 std 较小时，仍难以缓解。我认为原因可能是 \(\lVert\nabla W_2\rVert_F \approx N \sigma^3\)这个近似成立的前提需要较大的 $\sigma$（详见前文推导），所以在初始化 std 较小的情况下，$\eta \propto \frac{1}{\sigma^2}$未必正确。</p> <h2 id="尝试-1-失败的原因probably">尝试 1 失败的原因（probably）</h2> <p>除了近似成立的区间，我们还可以这样分析。在我们论文里的设定下，把参数写成 $W=\sigma U$ 后，模型对应的风险是</p> \[\mathcal L_\sigma(U)=\sum_{k=1}^K p_k\underbrace{\big(\sigma^2 g(U;e_k)-1\big)^2}_{q_k}.\] <p>注意目标 “1” 固定不随 $\sigma$ 缩放（绝大部分情况下，真实标签也不随 $\sigma$ 缩放），这一步已经破坏了尺度不变性。</p> <p>对 $W$ 做梯度下降</p> \[W^{t+1}=W^t-\eta \nabla_W \mathcal L(W^t),\] <p>换成 $U^t=W^t/\sigma$，可以推得精确更新式</p> \[U^{t+1} =U^t-2\eta\sum_{k=1}^K p_k\big(\sigma^2 g(U^t;e_k)-1\big)\,\nabla_U g(U^t;e_k). \tag{★}\] <p>这里面有个不可消掉的因子：</p> \[(\sigma^2 g(U^t;e_k)-1).\] <p>它依赖于当前 $U^t$ 和样本 $k$，不是全局常数。总的来说，不可能用一个标量 $\eta(\sigma)$ 来做到对齐效果。$\eta\propto 1/\sigma^2$ 在某些 regime 会稍微好一点（因为它对齐了某个主要尺度，比如 NTK 主项）。但它不可能把所有 $\sigma$ 的训练变成同一个问题，仍会有系统性漂移与不稳定。</p> <h2 id="尝试-2-成功">尝试 2 (成功)</h2> <p>到底要怎样才能对齐所有 $\sigma$ 的data scaling 曲线？</p> <p>为了把原来输出自动带 $\sigma^2$的缩放在 forward 中抵消掉，我们令</p> \[f_\sigma(x;W)=\frac{1}{\sigma^2}W_2\phi(W_1x), \qquad W_i\sim\mathcal N(0,\sigma^2),\] <p>并令学习率 $\eta_\sigma\propto\sigma^2$。</p> <p>若写 $W=\sigma U$，此时就有</p> \[\begin{aligned} f_\sigma(x;\sigma U) &amp;=\frac1{\sigma^2}(\sigma U_2)\phi((\sigma U_1)x)\\ &amp;=\frac1{\sigma^2}(\sigma U_2)\cdot(\sigma\phi(U_1x))\\ &amp;=U_2\phi(U_1x)=:f_1(x;U). \end{aligned}\] <p>右边完全不含 $\sigma$。</p> <p>令目标函数</p> \[\mathcal J_\sigma(W)=\sum_{k=1}^K p_k\big(f_\sigma(e_k;W)-1\big)^2, \quad f_\sigma(x;W)=\sigma^{-2}W_2\phi(W_1x).\] <p>令 $U=W/\sigma$。则易得：</p> <ol> <li>目标函数本身不含 $\sigma$：</li> </ol> \[\mathcal J_\sigma(\sigma U)=\mathcal J_1(U).\] <ol> <li>梯度缩放（链式法则）：</li> </ol> \[\nabla_W\mathcal J_\sigma(\sigma U)=\frac1\sigma \nabla_U\mathcal J_1(U).\] <ol> <li>梯度下降轨迹：</li> </ol> <p>定义 $U^t:=W^t/\sigma$，则</p> \[U^{t+1}=\frac{W^{t+1}}{\sigma} =\frac{W^t}{\sigma}-\frac{\eta_\sigma}{\sigma}\nabla_W\mathcal J_\sigma(W^t) =U^t-\frac{\eta_\sigma}{\sigma}\nabla_W\mathcal J_\sigma(\sigma U^t).\] <p>代入 $\nabla_W\mathcal J_\sigma(\sigma U^t)=\frac{1}{\sigma}\nabla_U\mathcal J_1(U^t)$，得</p> \[U^{t+1} =U^t-\frac{\eta_\sigma}{\sigma}\cdot\frac{1}{\sigma}\nabla_U\mathcal J_1(U^t) =U^t-\frac{\eta_\sigma}{\sigma^2}\nabla_U\mathcal J_1(U^t).\] <p>这个时候关键的地方来了，我们要取 $\eta_\sigma=\sigma^2\eta_0$，才能得到</p> \[U^{t+1}=U^t-\eta_0\nabla_U\mathcal J_1(U^t),\] <blockquote> <p>推广到 momentum SGD，也同样严格等价。</p> </blockquote> <p>综上，我们需要设定</p> \[f_\sigma(x;W_1,W_2)\;:=\;\frac{1}{\sigma^2}\,W_2\,\phi(W_1x).\] <p>初始化：</p> \[W_1^{(0)},W_2^{(0)} \stackrel{\text{i.i.d.}}{\sim}\mathcal N(0,\sigma^2).\] <p>学习率选择：</p> \[\eta_\sigma = \sigma^2 \eta_0.\] <p>这样才能够让不同初始化 std 下的 loss-$D$ 曲线对齐。下面是实验结果</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure id="fig:std-slope"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/recons-480.webp 480w,/assets/img/post-02-01/recons-800.webp 800w,/assets/img/post-02-01/recons-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/recons.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">对齐后 empirical slope 与初始化 std 的关系</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure id="fig:all-curves-recon"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post-02-01/all-curves-recon-480.webp 480w,/assets/img/post-02-01/all-curves-recon-800.webp 800w,/assets/img/post-02-01/all-curves-recon-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post-02-01/all-curves-recon.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">对齐后不同初始化 std 下的 data scaling 曲线</figcaption> </figure> </div> </div> <p>这个结果可以很容易推广到多层relu网络。设网络满足：</p> <ul> <li>$h_0=x$</li> <li>$h_\ell=\phi(W_\ell h_{\ell-1})$, $\ell=1,\dots,K-1$</li> <li>$f(x;W)=W_K h_{K-1}$</li> </ul> <p>并且初始化时所有层 $W_\ell\sim \mathcal N(0,\sigma^2)$。定义归一化后的模型输出：</p> \[\tilde f_\sigma(x;W)=\frac{1}{\sigma^K}f(x;W).\] <p>只要选：</p> \[\eta_\sigma=\sigma^2 \eta_0\] <p>就可以同时对齐 forward 和 backward 过程。</p> <h2 id="引用">引用</h2> <p>如果您需要引用本文，请参考：</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zou2026scaling</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{如何对齐不同初始化大小下的 Data scaling 曲线}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zou, Jiaxuan}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{Jiaxuan's Blog}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://jiaxuanzou0714.github.io/blog/2026/data-scaling-and-std/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="deep-learning"/><category term="scaling-law"/><summary type="html"><![CDATA[研究了 data scaling 的 empirical slope 关于初始化 std 的关系，并提出一种简单方法来对齐不同初始化大小下的 data scaling 曲线]]></summary></entry><entry><title type="html">Can We Derive Scaling Law From First Principles?</title><link href="https://jiaxuanzou0714.github.io/blog/2025/scaling-law/" rel="alternate" type="text/html" title="Can We Derive Scaling Law From First Principles?"/><published>2025-12-30T00:00:00+00:00</published><updated>2025-12-30T00:00:00+00:00</updated><id>https://jiaxuanzou0714.github.io/blog/2025/scaling-law</id><content type="html" xml:base="https://jiaxuanzou0714.github.io/blog/2025/scaling-law/"><![CDATA[<p>This post redirects to the PDF file.</p>]]></content><author><name></name></author><category term="publications"/><category term="research"/><category term="scaling-law"/><category term="deep-learning"/><category term="pdf"/><summary type="html"><![CDATA[New research available. Click to read the full PDF.]]></summary></entry></feed>