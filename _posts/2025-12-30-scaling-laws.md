---
title: "Can We Derive Scaling Law From First Principles?"
date: 2025-12-30
permalink: /posts/2025/12/scaling-laws/
tags:
  - scaling law
  - deep learning
  - theory
---

We are excited to share our latest work titled **"Can We Derive Scaling Law From First Principles?"**.

In this work, we explore the theoretical foundations of scaling laws in deep learning. We attempt to derive these laws from first principles, offering new insights into why neural networks scale the way they do.

我们很高兴分享我们的最新工作 **"Can We Derive Scaling Law From First Principles?"**。

在这项工作中，我们探索了深度学习中缩放定律（Scaling Laws）的理论基础。我们尝试从第一性原理推导这些定律，为神经网络为何如此缩放提供了新的见解。

You can download and read the full paper here / 您可以在此处下载并阅读完整论文：

[Download PDF / 下载 PDF](/files/Can_We_Derive_Scaling_Law_From_First_Principles.pdf)

### Abstract / 摘要

(Please update this section with the actual abstract from the paper / 请用论文的实际摘要更新此部分)

Scaling laws have become a cornerstone in understanding the performance of large language models and other deep learning systems. However, a rigorous derivation from first principles has remained elusive. This paper bridges the gap between empirical observations and theoretical understanding.

缩放定律已成为理解大型语言模型和其他深度学习系统性能的基石。然而，从第一性原理进行的严格推导仍然难以捉摸。本文弥合了经验观察与理论理解之间的差距。
